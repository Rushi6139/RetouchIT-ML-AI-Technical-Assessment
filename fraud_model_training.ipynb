{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2df6ff0-be83-47da-a011-c7257a372cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "# =====================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "from sklearn.metrics import classification_report, average_precision_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feedc120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6362620, 11)\n",
      "Target distribution:\n",
      " isFraud\n",
      "0    0.998709\n",
      "1    0.001291\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Dataset\n",
    "# =====================================================\n",
    "df = pd.read_csv(\"data/raw/transactions.csv\")\n",
    "target_col = \"isFraud\"  # 👈 adjust if different\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Target distribution:\\n\", y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20a875ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3817572, 10)\n",
      "Validation shape: (1272524, 10)\n",
      "Test shape: (1272524, 10)\n"
     ]
    }
   ],
   "source": [
    "# 3. Train/Validation/Test Split\n",
    "# =====================================================\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6fae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocessing\n",
    "# =====================================================\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Models & Hyperparameters (reduced for speed)\n",
    "# =====================================================\n",
    "models = {\n",
    "    \"log_reg\": LogisticRegression(max_iter=100, random_state=42),\n",
    "    \"rf\": RandomForestClassifier(random_state=42),\n",
    "    \"xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"log_reg\": {\n",
    "        \"classifier__C\": [0.1, 1],\n",
    "        \"classifier__penalty\": [\"l2\"]\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"classifier__n_estimators\": [50, 100],\n",
    "        \"classifier__max_depth\": [5, 10]\n",
    "    },\n",
    "    \"xgb\": {\n",
    "       \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15db5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Tuning log_reg on subset...\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "✅ Best log_reg params: {'classifier__penalty': 'l2', 'classifier__C': 0.1}\n",
      "✅ Best PR-AUC (CV): 0.5731\n",
      "\n",
      "🔹 Tuning rf on subset...\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "✅ Best rf params: {'classifier__n_estimators': 100, 'classifier__max_depth': 5}\n",
      "✅ Best PR-AUC (CV): 0.2415\n",
      "\n",
      "🔹 Tuning xgb on subset...\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\RetouchIT-ML-AI-Technical-Assessment\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:53:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best xgb params: {'classifier__subsample': 0.8, 'classifier__n_estimators': 100, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.2}\n",
      "✅ Best PR-AUC (CV): 0.8848\n"
     ]
    }
   ],
   "source": [
    "# 6. Hyperparameter Tuning on Subset\n",
    "# =====================================================\n",
    "# Subset for faster tuning\n",
    "X_tune, _, y_tune, _ = train_test_split(\n",
    "    X_train, y_train, test_size=0.7, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "X_tune_sub, X_es, y_tune_sub, y_es = train_test_split(\n",
    "    X_tune, y_tune, test_size=0.2, stratify=y_tune, random_state=42\n",
    ")\n",
    "\n",
    "best_params_all = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔹 Tuning {name} on subset...\")\n",
    "\n",
    "    # Build pipeline with the current model\n",
    "    pipe = ImbPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_grid[name],\n",
    "        n_iter=2,   # very small for speed\n",
    "        scoring=\"average_precision\",\n",
    "        cv=2,       # fewer folds\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # ✅ Do NOT pass eval_set here\n",
    "    search.fit(X_tune_sub, y_tune_sub)\n",
    "\n",
    "    print(f\"✅ Best {name} params: {search.best_params_}\")\n",
    "    print(f\"✅ Best PR-AUC (CV): {search.best_score_:.4f}\")\n",
    "\n",
    "    best_params_all[name] = search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321446fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dbefb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Retraining log_reg on FULL data...\n",
      "Val PR-AUC: 0.5945\n",
      "Val ROC-AUC: 0.9889\n",
      "\n",
      "🚀 Retraining rf on FULL data...\n",
      "Val PR-AUC: 0.0021\n",
      "Val ROC-AUC: 0.6695\n",
      "\n",
      "🚀 Retraining xgb on FULL data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\RetouchIT-ML-AI-Technical-Assessment\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:10:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val PR-AUC: 0.9070\n",
      "Val ROC-AUC: 0.9978\n"
     ]
    }
   ],
   "source": [
    "# 7. Retrain on FULL Training Data with Best Params\n",
    "# =====================================================\n",
    "final_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🚀 Retraining {name} on FULL data...\")\n",
    "\n",
    "    best_params = {\n",
    "        k.replace(\"classifier__\", \"\"): v\n",
    "        for k, v in best_params_all[name].items()\n",
    "    }\n",
    "    model.set_params(**best_params)\n",
    "\n",
    "    pipe = ImbPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    final_models[name] = pipe\n",
    "\n",
    "    # Validation eval\n",
    "    y_val_pred = pipe.predict_proba(X_val)[:, 1]\n",
    "    print(f\"Val PR-AUC: {average_precision_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"Val ROC-AUC: {roc_auc_score(y_val, y_val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ac9422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Final Test Evaluation ================\n",
      "\n",
      "⭐ Model: log_reg\n",
      "Test PR-AUC: 0.6097\n",
      "Test ROC-AUC: 0.9889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1270882\n",
      "           1       0.23      0.73      0.35      1642\n",
      "\n",
      "    accuracy                           1.00   1272524\n",
      "   macro avg       0.62      0.86      0.68   1272524\n",
      "weighted avg       1.00      1.00      1.00   1272524\n",
      "\n",
      "\n",
      "⭐ Model: rf\n",
      "Test PR-AUC: 0.0020\n",
      "Test ROC-AUC: 0.6697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.34      0.51   1270882\n",
      "           1       0.00      1.00      0.00      1642\n",
      "\n",
      "    accuracy                           0.34   1272524\n",
      "   macro avg       0.50      0.67      0.26   1272524\n",
      "weighted avg       1.00      0.34      0.51   1272524\n",
      "\n",
      "\n",
      "⭐ Model: xgb\n",
      "Test PR-AUC: 0.9124\n",
      "Test ROC-AUC: 0.9993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99   1270882\n",
      "           1       0.09      1.00      0.17      1642\n",
      "\n",
      "    accuracy                           0.99   1272524\n",
      "   macro avg       0.55      0.99      0.58   1272524\n",
      "weighted avg       1.00      0.99      0.99   1272524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Final Test Evaluation\n",
    "# =============================================== ======\n",
    "print(\"\\n================ Final Test Evaluation ================\")\n",
    "for name, model in final_models.items():\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"\\n⭐ Model: {name}\")\n",
    "    print(f\"Test PR-AUC: {average_precision_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_pred):.4f}\")\n",
    "    print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88b44249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final model saved as fraud_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# 9. Save Final Best Model\n",
    "# =====================================================\n",
    "final_model = final_models[\"xgb\"]  # 👈 pick best based on metrics\n",
    "joblib.dump(final_model, \"fraud_model.pkl\")\n",
    "print(\"\\n✅ Final model saved as fraud_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RetouchIT-ML-AI-Technical-Assessment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
